{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ISA 414 Final Project - Stock Predictions through Unsupervised Sentiment Analysis\n",
    "##### Ethan Buege, Jorge Nadjar, Mac Magyaros\n",
    "\n",
    "Libraries being used:\n",
    "- Pandas\n",
    "- Numpy\n",
    "- PSAW (Python Pushshift.io API Wrapper)\n",
    "- PRAW (Python Reddit API Wrapper)\n",
    "- PyTorch\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# standard python libraries\n",
    "import requests\n",
    "from requests import auth\n",
    "import time\n",
    "from time import sleep\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "import collections\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import pymongo\n",
    "\n",
    "# reddit API\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# unsupervised sentiment analysis - doc2vec encoding\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext as tt\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "top_10_tickers = ['GME', 'AMC', 'TSLA', 'AAPL', 'AMZN', 'AMD', 'NVDA', 'MSFT', 'SPY', 'QQQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc2vec_model = pickle.load(open('doc2vec_model.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated and vectorized GME posts saved to disk\n",
      "aggregated and vectorized AMC posts saved to disk\n",
      "aggregated and vectorized TSLA posts saved to disk\n",
      "aggregated and vectorized AAPL posts saved to disk\n",
      "aggregated and vectorized AMZN posts saved to disk\n",
      "aggregated and vectorized AMD posts saved to disk\n",
      "aggregated and vectorized NVDA posts saved to disk\n",
      "aggregated and vectorized MSFT posts saved to disk\n",
      "aggregated and vectorized SPY posts saved to disk\n",
      "aggregated and vectorized QQQ posts saved to disk\n"
     ]
    }
   ],
   "source": [
    "def aggregate_posts_by_day(ticker):\n",
    "    aggregated_daily_posts = {}\n",
    "    daily_upvotes = {}\n",
    "    daily_volume = {}\n",
    "\n",
    "    posts = pickle.load(open(f'E:/reddit_data/{ticker}_series.p', \"rb\"))\n",
    "    # replace UTC timestamp with date for easy aggregation\n",
    "    for post in posts:\n",
    "        day = dt.datetime.utcfromtimestamp(post.created_utc).date()\n",
    "        delta = day.weekday() - 4\n",
    "        if delta > 0:  # if day is weekend, aggregate to friday\n",
    "            days_back = dt.timedelta(delta)\n",
    "            day -= days_back\n",
    "\n",
    "        if day not in aggregated_daily_posts:\n",
    "            aggregated_daily_posts[day] = ''\n",
    "            daily_upvotes[day] = 0\n",
    "            daily_volume[day] = 0\n",
    "\n",
    "\n",
    "        if 'deleted by user' not in post.title and len(post.title) > 5:\n",
    "            aggregated_daily_posts[day] = ' '.join([aggregated_daily_posts[day], post.title])\n",
    "            daily_upvotes[day] += post.score\n",
    "            daily_volume[day] += 1\n",
    "            # add bodytext if it exists and has not been deleted\n",
    "            # WSB has minimum length requirements so the only posts less than 20 chars have been deleted\n",
    "            if len(post.selftext) > 20:\n",
    "                aggregated_daily_posts[day] = ' '.join([aggregated_daily_posts[day], post.selftext])\n",
    "\n",
    "    return aggregated_daily_posts, daily_upvotes, daily_volume\n",
    "\n",
    "\n",
    "# daily_posts = aggregate_posts_by_day('TSLA')\n",
    "\n",
    "\n",
    "for ticker in top_10_tickers:\n",
    "    daily_posts, daily_upvotes, daily_volume = aggregate_posts_by_day(ticker)\n",
    "    for day in daily_posts:\n",
    "        tokens = gensim.utils.simple_preprocess(daily_posts[day])\n",
    "        daily_posts[day] = doc2vec_model.infer_vector(tokens)\n",
    "\n",
    "    print(f'aggregated and vectorized {ticker} posts saved to disk')\n",
    "    # this is terribly inefficient but I needed to get this thing finished\n",
    "    pickle.dump(daily_posts, open(f'E:/reddit_data/{ticker}_vectorized_posts.p', \"wb\"))\n",
    "    pickle.dump(daily_upvotes, open(f'E:/reddit_data/{ticker}_daily_upvotes.p', \"wb\"))\n",
    "    pickle.dump(daily_volume, open(f'E:/reddit_data/{ticker}_daily_volume.p', \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'TSLA'\n",
    "\n",
    "vectorized_posts = pickle.load(open(f'E:/reddit_data/{ticker}_vectorized_posts.p', \"rb\"))\n",
    "daily_upvotes = pickle.load(open(f'E:/reddit_data/{ticker}_daily_upvotes.p', \"rb\"))\n",
    "daily_volume = pickle.load(open(f'E:/reddit_data/{ticker}_daily_volume.p', \"rb\"))\n",
    "prices = pd.read_csv(f'{ticker}_prices.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Addition - Jorge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            from      open      high       low     close    volume  \\\n0     2017-04-24    61.844    62.110   61.2043    61.606  25417525   \n1     2017-04-25    61.600    62.796   61.1720    62.758  33688540   \n2     2017-04-26    62.474    62.900   61.8000    62.034  23475220   \n3     2017-04-27    62.338    62.618   61.5000    61.726  17342845   \n4     2017-04-28    61.966    62.960   61.6000    62.814  22527390   \n...          ...       ...       ...       ...       ...       ...   \n1255  2022-04-18   989.030  1014.920  973.4100  1004.290  17237387   \n1256  2022-04-19  1005.060  1034.940  995.3250  1028.150  16604744   \n1257  2022-04-20  1030.000  1034.000  975.2501   977.200  23534922   \n1258  2022-04-21  1074.730  1092.220  996.4150  1008.780  35136565   \n1259  2022-04-22  1014.910  1034.850  994.0001  1005.050  23226886   \n\n      afterHours  preMarket  \\\n0        61.5460     62.000   \n1        62.7500     61.700   \n2        62.0020     62.628   \n3        61.8200     61.980   \n4        62.9200     61.958   \n...          ...        ...   \n1255   1013.0200    987.250   \n1256   1018.0000   1008.180   \n1257   1031.1800   1015.240   \n1258   1011.3999   1031.610   \n1259   1000.3000   1011.400   \n\n                                                 docvec  upvotes  post volume  \n0     [1.7273818, 2.0063844, -0.099462226, -0.209076...    167.0          1.0  \n1                                                   NaN      NaN          NaN  \n2                                                   NaN      NaN          NaN  \n3     [-1.8340168, 3.2863724, -1.204684, 1.9058441, ...     43.0          2.0  \n4                                                   NaN      NaN          NaN  \n...                                                 ...      ...          ...  \n1255                                                NaN      NaN          NaN  \n1256                                                NaN      NaN          NaN  \n1257                                                NaN      NaN          NaN  \n1258                                                NaN      NaN          NaN  \n1259                                                NaN      NaN          NaN  \n\n[1260 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>from</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>afterHours</th>\n      <th>preMarket</th>\n      <th>docvec</th>\n      <th>upvotes</th>\n      <th>post volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-04-24</td>\n      <td>61.844</td>\n      <td>62.110</td>\n      <td>61.2043</td>\n      <td>61.606</td>\n      <td>25417525</td>\n      <td>61.5460</td>\n      <td>62.000</td>\n      <td>[1.7273818, 2.0063844, -0.099462226, -0.209076...</td>\n      <td>167.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-04-25</td>\n      <td>61.600</td>\n      <td>62.796</td>\n      <td>61.1720</td>\n      <td>62.758</td>\n      <td>33688540</td>\n      <td>62.7500</td>\n      <td>61.700</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-04-26</td>\n      <td>62.474</td>\n      <td>62.900</td>\n      <td>61.8000</td>\n      <td>62.034</td>\n      <td>23475220</td>\n      <td>62.0020</td>\n      <td>62.628</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-04-27</td>\n      <td>62.338</td>\n      <td>62.618</td>\n      <td>61.5000</td>\n      <td>61.726</td>\n      <td>17342845</td>\n      <td>61.8200</td>\n      <td>61.980</td>\n      <td>[-1.8340168, 3.2863724, -1.204684, 1.9058441, ...</td>\n      <td>43.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-04-28</td>\n      <td>61.966</td>\n      <td>62.960</td>\n      <td>61.6000</td>\n      <td>62.814</td>\n      <td>22527390</td>\n      <td>62.9200</td>\n      <td>61.958</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1255</th>\n      <td>2022-04-18</td>\n      <td>989.030</td>\n      <td>1014.920</td>\n      <td>973.4100</td>\n      <td>1004.290</td>\n      <td>17237387</td>\n      <td>1013.0200</td>\n      <td>987.250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1256</th>\n      <td>2022-04-19</td>\n      <td>1005.060</td>\n      <td>1034.940</td>\n      <td>995.3250</td>\n      <td>1028.150</td>\n      <td>16604744</td>\n      <td>1018.0000</td>\n      <td>1008.180</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1257</th>\n      <td>2022-04-20</td>\n      <td>1030.000</td>\n      <td>1034.000</td>\n      <td>975.2501</td>\n      <td>977.200</td>\n      <td>23534922</td>\n      <td>1031.1800</td>\n      <td>1015.240</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1258</th>\n      <td>2022-04-21</td>\n      <td>1074.730</td>\n      <td>1092.220</td>\n      <td>996.4150</td>\n      <td>1008.780</td>\n      <td>35136565</td>\n      <td>1011.3999</td>\n      <td>1031.610</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1259</th>\n      <td>2022-04-22</td>\n      <td>1014.910</td>\n      <td>1034.850</td>\n      <td>994.0001</td>\n      <td>1005.050</td>\n      <td>23226886</td>\n      <td>1000.3000</td>\n      <td>1011.400</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1260 rows Ã— 11 columns</p>\n</div>"
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_values = vectorized_posts.items()\n",
    "posts_string = {key.strftime('%Y-%m-%d'): value for key, value in keys_values}\n",
    "prices['docvec'] = prices['from'].map(posts_string)\n",
    "\n",
    "keys_values = daily_upvotes.items()\n",
    "posts_string = {key.strftime('%Y-%m-%d'): value for key, value in keys_values}\n",
    "prices['upvotes'] = prices['from'].map(posts_string)\n",
    "\n",
    "keys_values = daily_volume.items()\n",
    "posts_string = {key.strftime('%Y-%m-%d'): value for key, value in keys_values}\n",
    "prices['post volume'] = prices['from'].map(posts_string)\n",
    "\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 1)\n",
      "(1260, 1)\n",
      "(1260, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_19972/3627139514.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price['close'] = scaler.fit_transform(price['close'].values.reshape(-1, 1))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_19972/3627139514.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  upvotes['upvotes'] = scaler.fit_transform(upvotes['upvotes'].values.reshape(-1, 1))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_19972/3627139514.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  volume['post volume'] = scaler.fit_transform(volume['post volume'].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "price = prices[['close']]\n",
    "price['close'] = scaler.fit_transform(price['close'].values.reshape(-1, 1))\n",
    "upvotes = prices[['upvotes']]\n",
    "upvotes['upvotes'] = scaler.fit_transform(upvotes['upvotes'].values.reshape(-1, 1))\n",
    "upvotes = upvotes.fillna(0)\n",
    "volume = prices[['post volume']]\n",
    "volume['post volume'] = scaler.fit_transform(volume['post volume'].values.reshape(-1, 1))\n",
    "volume = volume.fillna(0)\n",
    "print(price.shape)\n",
    "print(upvotes.shape)\n",
    "print(volume.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 125)\n",
      "[ 8.61668468e-01  1.00000012e+00 -4.40952964e-02 -9.84426737e-02\n",
      "  6.15872979e-01  1.91651747e-01 -9.96173620e-02 -3.80461156e-01\n",
      " -1.00000000e+00 -4.48948443e-01  5.06017208e-01  2.35546067e-01\n",
      "  1.11667261e-01  5.55926681e-01  6.25601351e-01  1.70470059e-01\n",
      " -8.32583964e-01  2.65944541e-01  1.02440618e-01  2.44176686e-01\n",
      " -2.51006693e-01  4.90468293e-02 -3.71593028e-01 -3.84453803e-01\n",
      "  7.51768351e-01  4.09332812e-01  2.29429919e-04  6.52647197e-01\n",
      "  5.13027841e-03  7.34517816e-04 -4.40690875e-01 -4.76047248e-01\n",
      " -9.19867977e-02  5.82740664e-01 -6.19729638e-01  1.54287577e-01\n",
      " -2.92563379e-01  9.25148278e-02 -8.39719027e-02  4.03524965e-01\n",
      "  1.22637525e-01  3.09875458e-01  1.81610510e-02  2.61289746e-01\n",
      "  1.35278940e-01  3.30417573e-01 -3.04801762e-01  1.28914893e-01\n",
      "  6.11102760e-01 -8.70962664e-02  2.80238628e-01 -4.47457701e-01\n",
      " -7.01964080e-01 -3.16663474e-01  1.25908270e-01  5.36438882e-01\n",
      "  9.74198580e-01  1.44917414e-01  2.15309352e-01 -4.52265799e-01\n",
      " -3.29403847e-01  5.12896217e-02 -2.72628605e-01  2.46965811e-01\n",
      " -1.59957409e-01  9.27215815e-02 -8.88608620e-02 -1.58332244e-01\n",
      "  3.60888958e-01 -9.30302888e-02  2.12791488e-01 -2.83972234e-01\n",
      " -1.66518632e-02 -1.96402460e-01 -1.30480155e-02  5.89756444e-02\n",
      " -3.82973611e-01  5.61011657e-02 -1.28069207e-01  9.79400396e-01\n",
      "  4.90479469e-02 -6.73414946e-01 -5.39005995e-02  2.65695810e-01\n",
      " -1.57613486e-01 -7.66731739e-01  9.11885500e-02  3.60649735e-01\n",
      " -1.75204247e-01 -2.33265996e-01  2.67091662e-01 -4.06875521e-01\n",
      "  2.49046922e-01 -2.38224313e-01  4.26663995e-01  1.35862365e-01\n",
      "  5.18861115e-01 -1.99264642e-02 -3.43294218e-02  4.23451960e-02\n",
      " -2.42279157e-01 -2.59763300e-01  9.44180265e-02  3.56604308e-01\n",
      " -3.45103711e-01  1.06757089e-01  3.00400883e-01  2.38251954e-01\n",
      "  1.30350843e-01 -2.07547933e-01  3.12116057e-01 -4.86797154e-01\n",
      "  4.73369032e-01 -4.52741385e-02  1.70628458e-01  2.67627925e-01\n",
      "  2.20841154e-01 -3.23171854e-01 -4.87587899e-01  3.13950330e-01\n",
      "  6.64270103e-01  9.68987495e-02 -3.03572088e-01  4.77420270e-01\n",
      "  2.45273069e-01]\n"
     ]
    }
   ],
   "source": [
    "n = len(price)\n",
    "price_list = price['close'].tolist()\n",
    "docscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "docvecs = prices['docvec'].to_numpy()\n",
    "for i in range(n):\n",
    "    if not isinstance(docvecs[i], (list,pd.Series,np.ndarray)):\n",
    "        docvecs[i] = np.zeros(125)\n",
    "    else:\n",
    "        # this method of normalization likely loses some some data contained in doc embeddings since it is refit for\n",
    "        # every sample but I didn't have enough time left figure out a more thorough solution\n",
    "        docvecs[i] = docscaler.fit_transform(docvecs[i].reshape(-1, 1)).reshape(125,)\n",
    "\n",
    "# docvecs = docvecs.reshape(1260, 300)\n",
    "docvecs = docvecs.tolist()\n",
    "docvecs = np.array(docvecs)\n",
    "print(docvecs.shape)\n",
    "print(docvecs[0])\n",
    "# print(docvecs[1])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 128)\n",
      "[-9.56768019e-01 -9.96803675e-01 -9.91735537e-01  8.61668468e-01\n",
      "  1.00000012e+00 -4.40952964e-02 -9.84426737e-02  6.15872979e-01\n",
      "  1.91651747e-01 -9.96173620e-02 -3.80461156e-01 -1.00000000e+00\n",
      " -4.48948443e-01  5.06017208e-01  2.35546067e-01  1.11667261e-01\n",
      "  5.55926681e-01  6.25601351e-01  1.70470059e-01 -8.32583964e-01\n",
      "  2.65944541e-01  1.02440618e-01  2.44176686e-01 -2.51006693e-01\n",
      "  4.90468293e-02 -3.71593028e-01 -3.84453803e-01  7.51768351e-01\n",
      "  4.09332812e-01  2.29429919e-04  6.52647197e-01  5.13027841e-03\n",
      "  7.34517816e-04 -4.40690875e-01 -4.76047248e-01 -9.19867977e-02\n",
      "  5.82740664e-01 -6.19729638e-01  1.54287577e-01 -2.92563379e-01\n",
      "  9.25148278e-02 -8.39719027e-02  4.03524965e-01  1.22637525e-01\n",
      "  3.09875458e-01  1.81610510e-02  2.61289746e-01  1.35278940e-01\n",
      "  3.30417573e-01 -3.04801762e-01  1.28914893e-01  6.11102760e-01\n",
      " -8.70962664e-02  2.80238628e-01 -4.47457701e-01 -7.01964080e-01\n",
      " -3.16663474e-01  1.25908270e-01  5.36438882e-01  9.74198580e-01\n",
      "  1.44917414e-01  2.15309352e-01 -4.52265799e-01 -3.29403847e-01\n",
      "  5.12896217e-02 -2.72628605e-01  2.46965811e-01 -1.59957409e-01\n",
      "  9.27215815e-02 -8.88608620e-02 -1.58332244e-01  3.60888958e-01\n",
      " -9.30302888e-02  2.12791488e-01 -2.83972234e-01 -1.66518632e-02\n",
      " -1.96402460e-01 -1.30480155e-02  5.89756444e-02 -3.82973611e-01\n",
      "  5.61011657e-02 -1.28069207e-01  9.79400396e-01  4.90479469e-02\n",
      " -6.73414946e-01 -5.39005995e-02  2.65695810e-01 -1.57613486e-01\n",
      " -7.66731739e-01  9.11885500e-02  3.60649735e-01 -1.75204247e-01\n",
      " -2.33265996e-01  2.67091662e-01 -4.06875521e-01  2.49046922e-01\n",
      " -2.38224313e-01  4.26663995e-01  1.35862365e-01  5.18861115e-01\n",
      " -1.99264642e-02 -3.43294218e-02  4.23451960e-02 -2.42279157e-01\n",
      " -2.59763300e-01  9.44180265e-02  3.56604308e-01 -3.45103711e-01\n",
      "  1.06757089e-01  3.00400883e-01  2.38251954e-01  1.30350843e-01\n",
      " -2.07547933e-01  3.12116057e-01 -4.86797154e-01  4.73369032e-01\n",
      " -4.52741385e-02  1.70628458e-01  2.67627925e-01  2.20841154e-01\n",
      " -3.23171854e-01 -4.87587899e-01  3.13950330e-01  6.64270103e-01\n",
      "  9.68987495e-02 -3.03572088e-01  4.77420270e-01  2.45273069e-01]\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate((price, upvotes, volume, docvecs), axis=1)\n",
    "print(data.shape)\n",
    "print(data[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Slice data and construct training/test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape =  torch.Size([992, 1])\n",
      "y_test.shape =  torch.Size([248, 1])\n"
     ]
    }
   ],
   "source": [
    "def split_data(stock_data, lookback):\n",
    "    data_raw = np.array(stock_data)  # convert to numpy array\n",
    "    data = []\n",
    "\n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - lookback):\n",
    "        data.append(data_raw[index: index + lookback])\n",
    "\n",
    "    data = np.array(data)\n",
    "    test_set_size = int(np.round(0.2 * data.shape[0]))\n",
    "    train_set_size = data.shape[0] - (test_set_size)\n",
    "\n",
    "    x_train = data[:train_set_size, :-1, :]\n",
    "    y_train = data[:train_set_size, -1, :1]\n",
    "\n",
    "    x_test = data[train_set_size:, :-1]\n",
    "    y_test = data[train_set_size:, -1, :1]\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "lookback = 20  # lookback sequence length\n",
    "x_train, y_train, x_test, y_test = split_data(data, lookback)\n",
    "\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test[:-1]).type(torch.Tensor)\n",
    "y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "print('y_train.shape = ',y_train_lstm.shape)\n",
    "print('y_test.shape = ',y_test_lstm.shape)\n",
    "\n",
    "# y_train_gru = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "# y_test_gru = torch.from_numpy(y_test).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create categorical y-vectors where y=0 if stock price went down on the following day, and y=1 if it went up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n",
      "[0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0\n",
      " 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1\n",
      " 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0\n",
      " 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
      " 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
      " 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0\n",
      " 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0\n",
      " 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0\n",
      " 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1\n",
      " 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0\n",
      " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1\n",
      " 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0\n",
      " 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
      " 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
      " 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0]\n",
      "247\n",
      "[1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
      " 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1\n",
      " 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0\n",
      " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1]\n",
      "x_train.shape =  torch.Size([992, 19, 128])\n",
      "y_train.shape =  torch.Size([992, 1])\n",
      "x_test.shape =  torch.Size([247, 19, 128])\n",
      "y_test.shape =  torch.Size([247, 1])\n"
     ]
    }
   ],
   "source": [
    "y_train_categorical = np.array([int(y_train[i+1] > y_train[i]) for i in range(len(y_train_lstm) - 1)])\n",
    "y_train_categorical = np.append(y_train_categorical, int(y_test[0] > y_train[-1]))\n",
    "print(len(y_train_categorical))\n",
    "print(y_train_categorical)\n",
    "\n",
    "y_test_categorical = np.array([int(y_test[i+1] > y_test[i]) for i in range(len(y_test) - 1)])\n",
    "print(len(y_test_categorical))\n",
    "print(y_test_categorical)\n",
    "\n",
    "y_train_categorical = torch.from_numpy(y_train_categorical).type(torch.Tensor).unsqueeze(-1)\n",
    "y_test_categorical = torch.from_numpy(y_test_categorical).type(torch.Tensor).unsqueeze(-1)\n",
    "\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ',y_train_categorical.shape)\n",
    "print('x_test.shape = ',x_test.shape)\n",
    "print('y_test.shape = ',y_test_categorical.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 128\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 125\n",
    "\n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "# criterion = torch.nn.MSELoss(reduction='mean')\n",
    "criterion = nn.BCELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train model over 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 BCE:  0.6947706937789917\n",
      "Epoch  1 BCE:  0.6935557723045349\n",
      "Epoch  2 BCE:  0.6925581693649292\n",
      "Epoch  3 BCE:  0.6917356252670288\n",
      "Epoch  4 BCE:  0.6910592913627625\n",
      "Epoch  5 BCE:  0.6905038356781006\n",
      "Epoch  6 BCE:  0.6900373697280884\n",
      "Epoch  7 BCE:  0.6896174550056458\n",
      "Epoch  8 BCE:  0.6891989707946777\n",
      "Epoch  9 BCE:  0.6887533068656921\n",
      "Epoch  10 BCE:  0.6882672905921936\n",
      "Epoch  11 BCE:  0.6877256035804749\n",
      "Epoch  12 BCE:  0.6871008276939392\n",
      "Epoch  13 BCE:  0.6863611936569214\n",
      "Epoch  14 BCE:  0.6854808926582336\n",
      "Epoch  15 BCE:  0.6844464540481567\n",
      "Epoch  16 BCE:  0.6832595467567444\n",
      "Epoch  17 BCE:  0.6819337010383606\n",
      "Epoch  18 BCE:  0.6804800629615784\n",
      "Epoch  19 BCE:  0.6788831353187561\n",
      "Epoch  20 BCE:  0.6770941615104675\n",
      "Epoch  21 BCE:  0.6750699281692505\n",
      "Epoch  22 BCE:  0.6728337407112122\n",
      "Epoch  23 BCE:  0.6704623699188232\n",
      "Epoch  24 BCE:  0.6680166721343994\n",
      "Epoch  25 BCE:  0.66551673412323\n",
      "Epoch  26 BCE:  0.6626513004302979\n",
      "Epoch  27 BCE:  0.6592583656311035\n",
      "Epoch  28 BCE:  0.6555117964744568\n",
      "Epoch  29 BCE:  0.6515781879425049\n",
      "Epoch  30 BCE:  0.6477096080780029\n",
      "Epoch  31 BCE:  0.6440209746360779\n",
      "Epoch  32 BCE:  0.6402102112770081\n",
      "Epoch  33 BCE:  0.6364220976829529\n",
      "Epoch  34 BCE:  0.6327893733978271\n",
      "Epoch  35 BCE:  0.6293896436691284\n",
      "Epoch  36 BCE:  0.6270250678062439\n",
      "Epoch  37 BCE:  0.6232576966285706\n",
      "Epoch  38 BCE:  0.6185806393623352\n",
      "Epoch  39 BCE:  0.6151320338249207\n",
      "Epoch  40 BCE:  0.6113109588623047\n",
      "Epoch  41 BCE:  0.6068440675735474\n",
      "Epoch  42 BCE:  0.6030092835426331\n",
      "Epoch  43 BCE:  0.5994332432746887\n",
      "Epoch  44 BCE:  0.5954632759094238\n",
      "Epoch  45 BCE:  0.5909590125083923\n",
      "Epoch  46 BCE:  0.586940348148346\n",
      "Epoch  47 BCE:  0.5836178064346313\n",
      "Epoch  48 BCE:  0.5804495215415955\n",
      "Epoch  49 BCE:  0.5776126980781555\n",
      "Epoch  50 BCE:  0.5710389018058777\n",
      "Epoch  51 BCE:  0.5668286085128784\n",
      "Epoch  52 BCE:  0.5649147033691406\n",
      "Epoch  53 BCE:  0.5594871044158936\n",
      "Epoch  54 BCE:  0.5545662641525269\n",
      "Epoch  55 BCE:  0.5516047477722168\n",
      "Epoch  56 BCE:  0.5479814410209656\n",
      "Epoch  57 BCE:  0.5432020425796509\n",
      "Epoch  58 BCE:  0.5382469892501831\n",
      "Epoch  59 BCE:  0.534930944442749\n",
      "Epoch  60 BCE:  0.5322464108467102\n",
      "Epoch  61 BCE:  0.5274224281311035\n",
      "Epoch  62 BCE:  0.521900475025177\n",
      "Epoch  63 BCE:  0.5169560313224792\n",
      "Epoch  64 BCE:  0.5131595134735107\n",
      "Epoch  65 BCE:  0.5102561712265015\n",
      "Epoch  66 BCE:  0.5053468346595764\n",
      "Epoch  67 BCE:  0.49969878792762756\n",
      "Epoch  68 BCE:  0.4939187169075012\n",
      "Epoch  69 BCE:  0.49019569158554077\n",
      "Epoch  70 BCE:  0.48695945739746094\n",
      "Epoch  71 BCE:  0.4816280007362366\n",
      "Epoch  72 BCE:  0.47578632831573486\n",
      "Epoch  73 BCE:  0.47040754556655884\n",
      "Epoch  74 BCE:  0.4662111699581146\n",
      "Epoch  75 BCE:  0.46268096566200256\n",
      "Epoch  76 BCE:  0.45893725752830505\n",
      "Epoch  77 BCE:  0.4546550512313843\n",
      "Epoch  78 BCE:  0.44717639684677124\n",
      "Epoch  79 BCE:  0.44128474593162537\n",
      "Epoch  80 BCE:  0.4379193186759949\n",
      "Epoch  81 BCE:  0.4336968660354614\n",
      "Epoch  82 BCE:  0.42798081040382385\n",
      "Epoch  83 BCE:  0.4208068549633026\n",
      "Epoch  84 BCE:  0.41505923867225647\n",
      "Epoch  85 BCE:  0.4110943078994751\n",
      "Epoch  86 BCE:  0.40715542435646057\n",
      "Epoch  87 BCE:  0.40318766236305237\n",
      "Epoch  88 BCE:  0.39537474513053894\n",
      "Epoch  89 BCE:  0.3873489499092102\n",
      "Epoch  90 BCE:  0.38154831528663635\n",
      "Epoch  91 BCE:  0.37786802649497986\n",
      "Epoch  92 BCE:  0.37492382526397705\n",
      "Epoch  93 BCE:  0.36813029646873474\n",
      "Epoch  94 BCE:  0.35983219742774963\n",
      "Epoch  95 BCE:  0.3521747291088104\n",
      "Epoch  96 BCE:  0.34812262654304504\n",
      "Epoch  97 BCE:  0.34548866748809814\n",
      "Epoch  98 BCE:  0.33884140849113464\n",
      "Epoch  99 BCE:  0.33083030581474304\n",
      "Epoch  100 BCE:  0.32319751381874084\n",
      "Epoch  101 BCE:  0.31890517473220825\n",
      "Epoch  102 BCE:  0.31583741307258606\n",
      "Epoch  103 BCE:  0.3089112937450409\n",
      "Epoch  104 BCE:  0.3009207844734192\n",
      "Epoch  105 BCE:  0.2931906282901764\n",
      "Epoch  106 BCE:  0.28816303610801697\n",
      "Epoch  107 BCE:  0.28466999530792236\n",
      "Epoch  108 BCE:  0.279239296913147\n",
      "Epoch  109 BCE:  0.27387535572052\n",
      "Epoch  110 BCE:  0.26408493518829346\n",
      "Epoch  111 BCE:  0.25603654980659485\n",
      "Epoch  112 BCE:  0.2510654032230377\n",
      "Epoch  113 BCE:  0.24693790078163147\n",
      "Epoch  114 BCE:  0.24373923242092133\n",
      "Epoch  115 BCE:  0.2346561998128891\n",
      "Epoch  116 BCE:  0.22565965354442596\n",
      "Epoch  117 BCE:  0.21787545084953308\n",
      "Epoch  118 BCE:  0.2132849544286728\n",
      "Epoch  119 BCE:  0.2111206203699112\n",
      "Epoch  120 BCE:  0.20427368581295013\n",
      "Epoch  121 BCE:  0.19885164499282837\n",
      "Epoch  122 BCE:  0.18786630034446716\n",
      "Epoch  123 BCE:  0.17984500527381897\n",
      "Epoch  124 BCE:  0.17612935602664948\n",
      "Training time: 28.890998363494873\n"
     ]
    }
   ],
   "source": [
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "    loss = criterion(y_train_pred, y_train_categorical)\n",
    "    print(\"Epoch \", t, \"BCE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time: {}\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Evaluate training and test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BCE:  1.374145746231079\n",
      "training accuracy: 0.96\n",
      "testing accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model(x_train)\n",
    "y_test_pred = model(x_test)\n",
    "\n",
    "test_loss = criterion(y_test_pred, y_test_categorical)\n",
    "print(\"Test BCE: \", test_loss.item())\n",
    "\n",
    "y_train_pred = torch.round(y_train_pred).detach().numpy()\n",
    "y_test_pred = torch.round(y_test_pred).detach().numpy()\n",
    "\n",
    "print(\"training accuracy: %.2f\" % (np.mean(y_train_pred == y_train_categorical.numpy())))\n",
    "print(\"testing accuracy: %.2f\" % (np.mean(y_test_pred == y_test_categorical.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [],
   "source": [
    "pickle.dump(model, open(f'{ticker}_RNN.p', \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}